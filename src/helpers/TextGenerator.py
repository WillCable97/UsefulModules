import tensorflow as tf
import numpy as np

from src.data.Text.Tokens.BaseClass import TokenBaseClass


from typing import Union

"""
    1- Input Handler: Takes input either as string or token and creates input to the model 
        (different sublasses account for different expectations in the formatting of the data assumed by the model)
        (Deals with tokeniser for all domains)
    
    2- Model Handler: Runs inputs through inderence pass
       Can determine latest token in the output

    3- Text Generator: Takes output from the model to keep track of the generated text
       (results are passed back to the input handler)
    
    
    Generated Text: Tracks text generated by the model and will maintain the output to be passed back into the next iteration of the model
       Deals with tokeniser of the output of the model 
       (takes into account the length of the content to ensure that there is no overflow being passed into model evaluation call)
"""


def string_to_tokens(input_string: str, tokeniser: TokenBaseClass, seq_len=None):
    tokenised_text = tokeniser.tokenise(input=[input_string], sequence_length=seq_len) #This returns slice dataset (can't be fed directly into model)
    numpy_text = [i.numpy() for i in tokenised_text]
    tensor_obj = tf.convert_to_tensor(numpy_text[0])
    return tensor_obj

class TextSequenceInputHandler:
    def __init__(self, string_beginning: Union[str, tf.Tensor], output_tokeniser: TokenBaseClass
                 , context_inputs: Union[str, list] = None, context_tokenisers: Union[TokenBaseClass, list] = None):
        self.string_beginning = string_beginning
        self.output_tokeniser = output_tokeniser
        self.context_inputs = context_inputs
        self.context_tokenisers = context_tokenisers

        self.single_domain = True if context_inputs is None else False

        self.auto_regressive_tensor = self.process_input(string_beginning, output_tokeniser)
        self.contexts = self.create_context()

    def process_input(self, input: Union[str,tf.Tensor], tokeniser: TokenBaseClass, seq_len = None):
        output = string_to_tokens(input, tokeniser, seq_len) if isinstance(input, str) else input
        return tf.expand_dims(output, axis=0)
    
    def create_context(self) -> list:
        if self.single_domain: return []
        
        inputs_int = None
        tokenisers_int = None

        if isinstance(self.context_inputs, str):
            inputs_int = [self.context_inputs]
            tokenisers_int = [self.context_tokenisers]
        else:
            inputs_int = self.context_inputs
            tokenisers_int = self.context_tokenisers

        zipped_inputs = zip(inputs_int, tokenisers_int)

        #Here we will assume context is padded to match model specs
        return [self.process_input(input_found, token_found, token_found.sequence_len) for input_found, token_found in zipped_inputs]

        
    def create_model_input(self):
        all_inputs = self.contexts
        all_inputs.append(self.auto_regressive_tensor)
        all_inputs = all_inputs[0] if self.single_domain else all_inputs
        return all_inputs
         


class ModelHandler:
    def __init__(self, input_model: tf.keras.Model):
        self.input_model = input_model
    
    def pass_inputs(self, inputs):
        return self.input_model(inputs)
    
    def get_new_token_by_max(self, input) -> int:
        output = self.pass_inputs(inputs=input)
        gen_distribution = output[0][-1]
        found_token = tf.argmax(gen_distribution)
        return found_token



class OutputSequence:
    def __init__(self, output_tokeniser: TokenBaseClass):
        self.output_tokeniser = output_tokeniser
        self.max_len_allowed = self.output_tokeniser.sequence_len
        self.init_complete = False
        self.initial_tokens: tf.Tensor = None
        self.generated_tokens = tf.convert_to_tensor([], dtype=tf.int32)

    def run_init(self, init_tokens: tf.Tensor):
        self.initial_tokens = init_tokens
        self.init_complete = True

    def add_token(self, new_token):
        self.generated_tokens = tf.concat([self.generated_tokens, [new_token]], axis=0)
    
    def create_new_input(self):
        full_tensor = tf.concat([self.initial_tokens[0], self.generated_tokens], axis=0)
        if tf.size(full_tensor) > self.max_len_allowed: full_tensor = full_tensor[(-1) * self.max_len_allowed:]
        return tf.expand_dims(full_tensor, axis =0)
    
    def print_results(self):
        print(self.output_tokeniser.detokenise(tf.expand_dims(self.generated_tokens, axis=0)))




class TextGenerator:
    def __init__(self, string_beginning: Union[str, tf.Tensor], output_tokeniser: TokenBaseClass
                 , input_model: tf.keras.Model
                 , context_inputs: Union[str, list] = None, context_tokenisers: Union[TokenBaseClass, list] = None):
        self.input_handle = TextSequenceInputHandler(string_beginning=string_beginning, output_tokeniser=output_tokeniser
                                                     , context_inputs=context_inputs, context_tokenisers=context_tokenisers)
        self.model_handle = ModelHandler(input_model=input_model)
        self.output_handle = OutputSequence(output_tokeniser)


    def generate_sequence(self, seqence_length):
        for i in range(seqence_length):
            model_input = self.input_handle.create_model_input()
            auto_regressive_input = self.input_handle.auto_regressive_tensor
            new_token = self.model_handle.get_new_token_by_max(model_input)
            if not self.output_handle.init_complete: self.output_handle.run_init(init_tokens=auto_regressive_input)
            self.output_handle.add_token(new_token=new_token)
            self.input_handle.auto_regressive_tensor = self.output_handle.create_new_input()


            #print(f"Initial Input: {self.input_handle.auto_regressive_tensor}")
            #print(f"Updated Input: {self.output_handle.create_new_input()}")
            

        self.output_handle.print_results()
        


